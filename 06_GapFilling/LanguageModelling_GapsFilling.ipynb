{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2709c659",
   "metadata": {},
   "source": [
    "# Language modelling\n",
    "\n",
    "The exercise shows how a language model may be used to solve word-prediction tasks and to generate text.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Read the documentation of [Language modelling in the Transformers](https://huggingface.co/transformers/task_summary.html#language-modeling) library.\n",
    "2. Download three [Polish models](https://huggingface.co/models?filter=pl) from the Huggingface repository. \n",
    "\n",
    "3. Produce the predictions for the following sentences (use each model and check 5 predictions):\n",
    "\n",
    "\n",
    "    i. (M) Warszawa to największe `[MASK]`.\n",
    "    ii. (D) Te zabawki należą do `[MASK]`.\n",
    "    iii. (C) Policjant przygląda się `[MASK]`.\n",
    "    iv. (B) Na środku skrzyżowania widać `[MASK]`.\n",
    "    v. (N) Właściciel samochodu widział złodzieja z `[MASK]`.\n",
    "    vi. (Ms) Prezydent z premierem rozmawiali wczoraj o `[MASK]`.\n",
    "    vii. (W) Witaj drogi `[MASK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70c0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10815a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb5114d29fd4787aa7b8ac1dcba4763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=229.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a90f843e15541dc9312f08db0a15506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e775b969ad4e6bbb583000f787922b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=906984.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5752ee8d83d3468082b017a78a3eb538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=555571.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee88c8989cb349f0bd35902ef67acd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=129.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442e40dd09d4f04aa1de883608e36fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=654201076.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "output = model(\n",
    "    **tokenizer.batch_encode_plus(\n",
    "        [\n",
    "            (\n",
    "                \"A potem szedł środkiem drogi w kurzawie, bo zamiatał nogami, ślepy dziad prowadzony przez tłustego kundla na sznurku.\",\n",
    "                \"A potem leciał od lasu chłopak z butelką, ale ten ujrzawszy księdza przy drodze okrążył go z dala i biegł na przełaj pól do karczmy.\"\n",
    "            )\n",
    "        ],\n",
    "    padding='longest',\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72985ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module transformers.models.cpm has no attribute CpmTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9f004a3884b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2141\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module transformers.models.cpm has no attribute CpmTokenizer"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import * \n",
    "pprint(unmasker(f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647c13a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import transformers\n",
    "[m[0] for m in inspect.getmembers(transformers, inspect.isclass) if m[1].__module__ == 'unmasker']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "pprint(unmasker(f\"HuggingFace is creating a {unmasker.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ce2334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForMaskedLM: ['cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "sequence = f\"Warszawa to największe {tokenizer.mask_token}.\"\n",
    "def predict(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(**inputs).logits\n",
    "\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    for token in top_5_tokens:\n",
    "        print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa75b00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warszawa to największe miasto.\n",
      "Warszawa to największe lotnisko.\n",
      "Warszawa to największe centrum.\n",
      "Warszawa to największe miasta.\n",
      "Warszawa to największe atrakcje.\n",
      "Te zabawki należą do rodziny.\n",
      "Te zabawki należą do nas.\n",
      "Te zabawki należą do nich.\n",
      "Te zabawki należą do najlepszych.\n",
      "Te zabawki należą do ..\n",
      "Policjant przygląda się mężczyźnie.\n",
      "Policjant przygląda się kobiecie.\n",
      "Policjant przygląda się mu.\n",
      "Policjant przygląda się dziewczynie.\n",
      "Policjant przygląda się sprawie.\n",
      "Policjant przygląda się mężczyźnie.\n",
      "Policjant przygląda się kobiecie.\n",
      "Policjant przygląda się mu.\n",
      "Policjant przygląda się dziewczynie.\n",
      "Policjant przygląda się sprawie.\n",
      "Na środku skrzyżowania widać rondo.\n",
      "Na środku skrzyżowania widać samochody.\n",
      "Na środku skrzyżowania widać radiowóz.\n",
      "Na środku skrzyżowania widać samochód.\n",
      "Na środku skrzyżowania widać wiadukt.\n",
      "Właściciel samochodu widział złodzieja z samochodu.\n",
      "Właściciel samochodu widział złodzieja z włamaniem.\n",
      "Właściciel samochodu widział złodzieja z auta.\n",
      "Właściciel samochodu widział złodzieja z kierowcą.\n",
      "Właściciel samochodu widział złodzieja z parkingu.\n",
      "Prezydent z premierem rozmawiali wczoraj o przyszłości.\n",
      "Prezydent z premierem rozmawiali wczoraj o Polsce.\n",
      "Prezydent z premierem rozmawiali wczoraj o bezpieczeństwie.\n",
      "Prezydent z premierem rozmawiali wczoraj o polityce.\n",
      "Prezydent z premierem rozmawiali wczoraj o Warszawie.\n",
      "Witaj drogi Łukasz.\n",
      "Witaj drogi Boże.\n",
      "Witaj drogi człowieku.\n",
      "Witaj drogi Karol.\n",
      "Witaj drogi Marcin.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predict(f\"Warszawa to największe {tokenizer.mask_token}.\")\n",
    "predict(f\"Te zabawki należą do {tokenizer.mask_token}.\")\n",
    "predict(f\"Policjant przygląda się {tokenizer.mask_token}.\")\n",
    "predict(f\"Policjant przygląda się {tokenizer.mask_token}.\")\n",
    "predict(f\"Na środku skrzyżowania widać {tokenizer.mask_token}.\")\n",
    "predict(f\"Właściciel samochodu widział złodzieja z {tokenizer.mask_token}.\")\n",
    "predict(f\"Prezydent z premierem rozmawiali wczoraj o {tokenizer.mask_token}.\")\n",
    "predict(f\"Witaj drogi {tokenizer.mask_token}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a32ff4",
   "metadata": {},
   "source": [
    "   \n",
    "4. Check the model predictions for the following sentences (using each model):\n",
    "\n",
    "\n",
    "    i. Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie `[MASK]`.\n",
    "    ii. Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie `[MASK]`.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a765fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie poddał.\n",
      "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zdziwił.\n",
      "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie dowiedział.\n",
      "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zastanawiał.\n",
      "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie przyznał.\n",
      "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie dowiedziała.\n",
      "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie przyznała.\n",
      "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie bała.\n",
      "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zmieniła.\n",
      "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zgodziła.\n"
     ]
    }
   ],
   "source": [
    "predict(f\"Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie {tokenizer.mask_token}.\")\n",
    "predict(f\"Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie {tokenizer.mask_token}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03192641",
   "metadata": {},
   "source": [
    "5. Check the model predictions for the following sentences:\n",
    "\n",
    "\n",
    "    i. `[MASK]` wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
    "    ii. W wakacje odwiedziłem `[MASK]`, który jest stolicą Islandii.\n",
    "    iii. Informatyka na `[MASK]` należy do najlepszych kierunków w Polsce.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ec7ae",
   "metadata": {},
   "source": [
    "6. If you want to use causal language models such as PapuGaPT2 or plT5, you should change the last three examples to accomodate for the fact, that these\n",
    "   models are better suited for causal language modelling.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9155a2",
   "metadata": {},
   "source": [
    "7. Answer the following questions:\n",
    "   \n",
    "   \n",
    "    i. Which of the models produced the best results?\n",
    "    ii. Was any of the models able to capture Polish grammar?\n",
    "    iii. Was any of the models able to capture long-distant relationships between the words?\n",
    "    iv. Was any of the models able to capture world knowledge?\n",
    "    v. What are the most striking errors made by the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9556fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
